{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Post and Comment Scraper Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial, you will learn how to use the Reddit API to scrape posts and comments from a subreddit. You will also learn how to export the scraped data to CSV files.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before we begin, you will need to install the following Python packages:\n",
    "\n",
    "* praw\n",
    "* tqdm\n",
    "* argparse\n",
    "\n",
    "You can install these packages using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw tqdm argparse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create a Reddit API instance\n",
    "\n",
    "The first step is to create a Reddit API instance using environment variables for authentication. We will define a function called create_reddit_instance() to create the instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "\n",
    "def create_reddit_instance():\n",
    "    \"\"\"\n",
    "    Create a Reddit API instance using environment variables for authentication.\n",
    "    :return: A Reddit API instance.\n",
    "    \"\"\"\n",
    "    return praw.Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=\"reddit-scraper\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace REDDIT_CLIENT_ID and REDDIT_CLIENT_SECRET with your own Reddit API credentials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Scrape posts from a subreddit\n",
    "\n",
    "Next, we will define a function called scrape_posts() to scrape posts from a subreddit using the Reddit API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_posts(reddit, subreddit_name: str, num_posts: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape posts from a subreddit using the Reddit API.\n",
    "    :param reddit: A Reddit API instance.\n",
    "    :param subreddit_name: The name of the subreddit to scrape.\n",
    "    :param num_posts: The number of posts to scrape.\n",
    "    :return: A list of dictionaries containing post data.\n",
    "    \"\"\"\n",
    "    print(\"Scraping posts...\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in tqdm(subreddit.hot(limit=num_posts), total=num_posts):\n",
    "        post_data = {\n",
    "            \"post_title\": post.title,\n",
    "            \"post_id\": post.id,\n",
    "            \"num_upvotes\": post.score,\n",
    "            \"tags\": post.link_flair_text,\n",
    "            \"post_content\": post.selftext,\n",
    "            \"post_sentiment\": analyze_sentiment(post.selftext),\n",
    "        }\n",
    "        posts_data.append(post_data)\n",
    "\n",
    "    return posts_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes three arguments: the Reddit API instance (reddit), the name of the subreddit to scrape (subreddit_name), and the number of posts to scrape (num_posts).\n",
    "\n",
    "The function first prints a message to indicate that it is scraping posts from the subreddit. It then uses the subreddit.hot() method to get the top posts in the subreddit, limited to the number of posts specified by num_posts.\n",
    "\n",
    "The function then iterates over each post and extracts the relevant data, including the post title, ID, number of upvotes, tags, content, and sentiment. The data is stored in a list of dictionaries called posts_data.\n",
    "\n",
    "Finally, the function returns the list of post data dictionaries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Scrape comments from posts\n",
    "\n",
    "Next, we will define a function called scrape_comments() to scrape comments from the posts that were scraped in the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(reddit, post_ids: List[str], num_comments: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Scrape comments from a list of Reddit posts.\n",
    "    :param reddit: A Reddit API instance.\n",
    "    :param post_ids: A list of post IDs to scrape comments from.\n",
    "    :param num_comments: The number of comments to scrape per post.\n",
    "    :return: A list of dictionaries containing comment data.\n",
    "    \"\"\"\n",
    "    print(\"Scraping comments...\")\n",
    "    comments_data = []\n",
    "\n",
    "    for post_id in tqdm(post_ids, desc=\"Posts\"):\n",
    "        post = reddit.submission(id=post_id)\n",
    "\n",
    "        post.comments.replace_more(limit=None)\n",
    "        for comment in tqdm(post.comments.list()[:num_comments], total=num_comments, desc=\"Comments\"):\n",
    "            comment_data = {\n",
    "                \"post_title\": post.title,\n",
    "                \"post_id\": post_id,\n",
    "                \"commenter_name\": comment.author.name,\n",
    "                \"comment_body\": comment.body,\n",
    "                \"num_upvotes\": comment.score,\n",
    "                \"comment_sentiment\": analyze_sentiment(comment.body),\n",
    "            }\n",
    "            comments_data.append(comment_data)\n",
    "\n",
    "    return comments_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scrape_comments() function takes in the Reddit instance, a list of post IDs, and the number of comments to scrape per post. It returns a list of dictionaries containing comment data.\n",
    "\n",
    "The function first initializes an empty list called comments_data to store the comment data. It then loops through each post ID and retrieves the submission object using the id method of the reddit instance.\n",
    "\n",
    "The replace_more method is called to ensure that all nested comments are included. The function then loops through each comment using the list() method and retrieves the comment data such as the author name, body, number of upvotes, and sentiment. The data is stored as a dictionary and appended to the comments_data list.\n",
    "\n",
    "We use the tqdm library to display a progress bar for both the posts and comments loops. This makes it easy to track the progress of the scraping process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 export data to CSV \n",
    "\n",
    "After scraping both posts and comments, we can use a function called export_data() to write the scraped data to separate CSV files. Here's an example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(posts: List[Dict], comments: List[Dict]):\n",
    "    \"\"\"\n",
    "    Export post and comment data to separate CSV files.\n",
    "    :param posts: A list of dictionaries containing post data.\n",
    "    :param comments: A list of dictionaries containing comment data.\n",
    "    \"\"\"\n",
    "    export_to_csv(posts, \"posts.csv\")\n",
    "    export_to_csv(comments, \"comments.csv\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the export_data() function, we use the previously defined export_to_csv() function to write the scraped post and comment data to separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 5 Running it all in Main\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function for the Reddit post and comment scraper script.\n",
    "    \"\"\"\n",
    "    subreddit, num_posts, num_comments = parse_args()\n",
    "    reddit = create_reddit_instance()\n",
    "    posts, comments = gather_data(reddit, subreddit, num_posts, num_comments)\n",
    "    export_data(posts, comments)\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will write the scraped post and comment data to CSV files named posts.csv and comments.csv, respectively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
