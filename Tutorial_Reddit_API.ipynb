{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Post and Comment Scraper\n",
    "\n",
    "This Jupyter Notebook is designed to scrape posts and comments from a specified subreddit using the Python Reddit API Wrapper (PRAW). We will be scraping the post title, post ID, number of upvotes, tags, and post content for each post, as well as the commenter's name, comment body, and number of upvotes for each comment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we begin, please make sure you have the following:\n",
    "\n",
    "1. A Reddit account\n",
    "2. A Reddit \"app\" created for API access (https://www.reddit.com/prefs/apps)\n",
    "3. Your `REDDIT_CLIENT_ID` and `REDDIT_CLIENT_SECRET` from the Reddit app\n",
    "4. The `PRAW` library installed (`pip install praw`)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Libraries\n",
    "\n",
    "First, let's import the required libraries.\n",
    "\n",
    "Importing a library in Python means loading and making available a set of pre-written code or modules that can be used to perform specific tasks. When you import a library, you gain access to a set of functions and classes that can be used in your own code. This allows you to take advantage of existing code instead of having to write everything from scratch.\n",
    "\n",
    "In this case we're using the Python Reddit API Wrapper (PRAW) to help us interact with the python reddit api more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install praw tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import praw\n",
    "import csv\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Reddit API Instance\n",
    "\n",
    "Now, let's create a function to initialize the Reddit API instance using the `REDDIT_CLIENT_ID` and `REDDIT_CLIENT_SECRET`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reddit_instance():\n",
    "    return praw.Reddit(\n",
    "        client_id=os.environ[\"REDDIT_CLIENT_ID\"],\n",
    "        client_secret=os.environ[\"REDDIT_CLIENT_SECRET\"],\n",
    "        user_agent=\"reddit-scraper\",\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Phone Calls metaphor, we can think of praw.Reddit as a phonebook that lists all the phone numbers we need to make a phone call to the Reddit API. The create_reddit_instance() function essentially looks up the phone number of the Reddit API from the phonebook and returns a connection to it.\n",
    "\n",
    "To make the connection, the function requires three pieces of information: the client_id, client_secret, and user_agent. These pieces of information are like the caller ID, password, and name you provide when making a phone call. In this case, they identify who you are and why you're making the API call, so that the API can verify your identity and grant you access to its data. Once the connection is established, you can start making API requests to the Reddit API, just like you can start making phone calls once you've established a connection to the phone number you looked up in the phonebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Posts\n",
    "\n",
    "Next, we'll create a function to scrape a specified number of posts from a subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_posts(reddit, subreddit_name: str, num_posts: int) -> List[Dict]:\n",
    "    print(\"Scraping posts...\")\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    posts_data = []\n",
    "\n",
    "    for post in tqdm(subreddit.hot(limit=num_posts), total=num_posts):\n",
    "        post_data = {\n",
    "            \"post_title\": post.title,\n",
    "            \"post_id\": post.id,\n",
    "            \"num_upvotes\": post.score,\n",
    "            \"tags\": post.link_flair_text,\n",
    "            \"post_content\": post.selftext,\n",
    "        }\n",
    "        posts_data.append(post_data)\n",
    "\n",
    "    return posts_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The function takes three arguments: reddit (an instance of the Reddit API), subreddit_name (a string representing the name of the subreddit to scrape), and num_posts (an integer representing the number of posts to scrape).\n",
    "\n",
    "2. The function first prints a message indicating that it is about to start scraping posts.\n",
    "\n",
    "3. The function then uses the subreddit_name argument to get a reference to the desired subreddit using the reddit.subreddit() method.\n",
    "\n",
    "4. The function initializes an empty list called posts_data to hold the scraped post data.\n",
    "\n",
    "5. The function then loops through the num_posts hottest posts on the subreddit (using the subreddit.hot() method) and for each post:\n",
    "\n",
    "\ta. The function extracts several pieces of information (the post title, post ID, number of upvotes, post tags, and post content) and stores them in a dictionary called post_data.\n",
    "\t\n",
    "\tb. The post_data dictionary is then appended to the posts_data list.\n",
    "\n",
    "6. Finally, the function returns the posts_data list, which contains dictionaries representing the scraped post data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Comments\n",
    "\n",
    "Now, let's create a function to scrape a specified number of comments from a list of Reddit posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_comments(reddit, post_ids: List[str], num_comments: int) -> List[Dict]:\n",
    "    print(\"Scraping comments...\")\n",
    "    comments_data = []\n",
    "\n",
    "    for post_id in tqdm(post_ids, desc=\"Posts\"):\n",
    "        post = reddit.submission(id=post_id)\n",
    "\n",
    "        post.comments.replace_more(limit=None)\n",
    "        for comment in tqdm(post.comments.list()[:num_comments], total=num_comments, desc=\"Comments\"):\n",
    "            comment_data = {\n",
    "                \"post_title\": post.title,\n",
    "                \"post_id\": post_id,\n",
    "                \"commenter_name\": comment.author.name,\n",
    "                \"comment_body\": comment.body,\n",
    "                \"num_upvotes\": comment.score,\n",
    "            }\n",
    "            comments_data.append(comment_data)\n",
    "\n",
    "    return comments_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define a function called scrape_comments() that takes three arguments: a Reddit instance (reddit), a list of post IDs (post_ids), and the number of comments to scrape per post (num_comments).\n",
    "\n",
    "2. Print the message \"Scraping comments...\" to the console to indicate that the function has started.\n",
    "\n",
    "3. Create an empty list called comments_data to store the scraped comment data.\n",
    "\n",
    "4. For each post ID in the list of post IDs:\n",
    "\t\n",
    "\t* Use the reddit.submission(id=post_id) method to get the post object corresponding to the current post ID.\n",
    "\t\n",
    "\t* Use the post.comments.replace_more(limit=None) method to retrieve all comments for the current post.\n",
    "\t\n",
    "\t* For each comment in the list of comments:\n",
    "\t\t\n",
    "\t\t* Create a dictionary called comment_data with the following keys and corresponding values:\n",
    "\t\t\n",
    "\t\t\t* \"post_title\": the title of the post that the comment was made on\n",
    "\t\t\t\n",
    "\t\t\t* \"post_id\": the ID of the post that the comment was made on\n",
    "\t\n",
    "\t\t\t* \"commenter_name\": the username of the commenter\n",
    "\n",
    "\t\t\t* \"comment_body\": the text content of the comment\n",
    "\n",
    "\t\t\t* \"num_upvotes\": the number of upvotes the comment has received\n",
    "\n",
    "\t\t\t* Append the comment_data dictionary to the comments_data list.\n",
    "\n",
    "5. Return the comments_data list containing all of the scraped comment data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data to CSV\n",
    "\n",
    "Next, we'll create a function to export the scraped data to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_csv(data: List[Dict], filename: str):\n",
    "    print(f\"Exporting data to {filename}...\")\n",
    "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a breakdown of the code:\n",
    "\n",
    "1. Define a function called export_to_csv that takes two arguments, data and filename.\n",
    "\n",
    "2. The function prints a message to the console indicating that it is exporting data to a CSV file with the specified filename.\n",
    "\n",
    "3. The function then uses the open function to create a new file with the specified filename in write mode, with the optional newline parameter set to \"\" to ensure that no extra newlines are added to the file.\n",
    "\n",
    "4. The csv.DictWriter function is used to create a writer object that writes rows to the CSV file. The fieldnames argument is set to the keys of the first dictionary in the data list, ensuring that all dictionaries have the same fields.\n",
    "\n",
    "5. The writeheader method is called on the writer object to write the header row to the CSV file.\n",
    "\n",
    "6. A loop is then used to iterate through each dictionary in the data list.\n",
    "\n",
    "7. The writerow method is called on the writer object for each dictionary in the data list, writing a new row to the CSV file.\n",
    "\n",
    "8. Finally, the function returns nothing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather and Export Data\n",
    "\n",
    "Finally, let's create a function to gather the post and comment data and export them to separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_and_export_data(subreddit: str, num_posts: int, num_comments: int):\n",
    "    reddit = create_reddit_instance()\n",
    "    \n",
    "    posts = scrape_posts(reddit, subreddit, num_posts)\n",
    "    comments = scrape_comments(reddit, [post[\"post_id\"] for post in posts], num_comments)\n",
    "\n",
    "    print(json.dumps(posts, indent=4))\n",
    "    \n",
    "    export_to_csv(posts, \"posts.csv\")\n",
    "    export_to_csv(comments, \"comments.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function called gather_and_export_data() that takes three arguments: subreddit (a string), num_posts (an integer), and num_comments (also an integer). Here's what the function does:\n",
    "\n",
    "1. It creates a Reddit API instance using the create_reddit_instance() function.\n",
    "\n",
    "2. It scrapes posts using the scrape_posts() function and saves the data to a variable called posts.\n",
    "\n",
    "3. It extracts the IDs of the posts from the posts variable and passes them to the scrape_comments() function, which scrapes the comments for each post and saves the data to a variable called comments.\n",
    "\n",
    "4. It exports the post data to a CSV file called \"posts.csv\" using the export_to_csv() function.\n",
    "\n",
    "5. It exports the comment data to a CSV file called \"comments.csv\" using the export_to_csv() function.\n",
    "\n",
    "In summary, this function scrapes data from a subreddit, gathers post and comment data, and exports the data to CSV files for further analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "To use the scraper, simply call the `gather_and_export_data` function with the desired subreddit name, number of posts to scrape, and number of comments to scrape per post.\n",
    "\n",
    "### Remember to add your REDDIT API KEY before running the next cell!\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_and_export_data(\"learnpython\", 10, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will scrape 10 posts and 5 comments for each post from the \"learnpython\" subreddit and save the data in \"posts.csv\" and \"comments.csv\" files."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
